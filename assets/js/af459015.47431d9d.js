"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[7518],{15680:(e,n,a)=>{a.d(n,{xA:()=>m,yg:()=>u});var t=a(96540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function l(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=t.createContext({}),p=function(e){var n=t.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},m=function(e){var n=p(e.components);return t.createElement(s.Provider,{value:n},e.children)},g="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},d=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),g=p(a),d=r,u=g["".concat(s,".").concat(d)]||g[d]||c[d]||i;return a?t.createElement(u,o(o({ref:n},m),{},{components:a})):t.createElement(u,o({ref:n},m))}));function u(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[g]="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=a[p];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}d.displayName="MDXCreateElement"},19365:(e,n,a)=>{a.d(n,{A:()=>o});var t=a(96540),r=a(20053);const i={tabItem:"tabItem_Ymn6"};function o(e){let{children:n,hidden:a,className:o}=e;return t.createElement("div",{role:"tabpanel",className:(0,r.A)(i.tabItem,o),hidden:a},n)}},11470:(e,n,a)=>{a.d(n,{A:()=>N});var t=a(58168),r=a(96540),i=a(20053),o=a(23104),l=a(56347),s=a(57485),p=a(31682),m=a(89466);function g(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:a,attributes:t,default:r}}=e;return{value:n,label:a,attributes:t,default:r}}))}function c(e){const{values:n,children:a}=e;return(0,r.useMemo)((()=>{const e=n??g(a);return function(e){const n=(0,p.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,a])}function d(e){let{value:n,tabValues:a}=e;return a.some((e=>e.value===n))}function u(e){let{queryString:n=!1,groupId:a}=e;const t=(0,l.W6)(),i=function(e){let{queryString:n=!1,groupId:a}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:n,groupId:a});return[(0,s.aZ)(i),(0,r.useCallback)((e=>{if(!i)return;const n=new URLSearchParams(t.location.search);n.set(i,e),t.replace({...t.location,search:n.toString()})}),[i,t])]}function y(e){const{defaultValue:n,queryString:a=!1,groupId:t}=e,i=c(e),[o,l]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!d({value:n,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const t=a.find((e=>e.default))??a[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i}))),[s,p]=u({queryString:a,groupId:t}),[g,y]=function(e){let{groupId:n}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(n),[t,i]=(0,m.Dv)(a);return[t,(0,r.useCallback)((e=>{a&&i.set(e)}),[a,i])]}({groupId:t}),f=(()=>{const e=s??g;return d({value:e,tabValues:i})?e:null})();(0,r.useLayoutEffect)((()=>{f&&l(f)}),[f]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!d({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),y(e)}),[p,y,i]),tabValues:i}}var f=a(92303);const h={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function v(e){let{className:n,block:a,selectedValue:l,selectValue:s,tabValues:p}=e;const m=[],{blockElementScrollPositionUntilNextRender:g}=(0,o.a_)(),c=e=>{const n=e.currentTarget,a=m.indexOf(n),t=p[a].value;t!==l&&(g(n),s(t))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const a=m.indexOf(e.currentTarget)+1;n=m[a]??m[0];break}case"ArrowLeft":{const a=m.indexOf(e.currentTarget)-1;n=m[a]??m[m.length-1];break}}n?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":a},n)},p.map((e=>{let{value:n,label:a,attributes:o}=e;return r.createElement("li",(0,t.A)({role:"tab",tabIndex:l===n?0:-1,"aria-selected":l===n,key:n,ref:e=>m.push(e),onKeyDown:d,onClick:c},o,{className:(0,i.A)("tabs__item",h.tabItem,o?.className,{"tabs__item--active":l===n})}),a??n)})))}function b(e){let{lazy:n,children:a,selectedValue:t}=e;const i=(Array.isArray(a)?a:[a]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===t));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},i.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))))}function C(e){const n=y(e);return r.createElement("div",{className:(0,i.A)("tabs-container",h.tabList)},r.createElement(v,(0,t.A)({},e,n)),r.createElement(b,(0,t.A)({},e,n)))}function N(e){const n=(0,f.A)();return r.createElement(C,(0,t.A)({key:String(n)},e))}},16773:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>m,contentTitle:()=>s,default:()=>u,frontMatter:()=>l,metadata:()=>p,toc:()=>g});var t=a(58168),r=(a(96540),a(15680)),i=a(11470),o=a(19365);const l={id:"Programming-paradigm",title:"Programming Paradigm",sidebar_position:1},s=void 0,p={unversionedId:"development/Programming-paradigm",id:"development/Programming-paradigm",title:"Programming Paradigm",description:"There are some rules and conventions to be followed in any framework. Only by following and mastering these rules can we use them more easily and achieve twice the result with half the effort.When we develop Flink job, we actually use the API provided by Flink to write an executable program (which must have a main() function) according to the development method required by Flink. We access variousConnectorin the program, and after a series of operatoroperations, we finally sink the data to the target storage through the Connector .",source:"@site/docs/development/model.md",sourceDirName:"development",slug:"/development/Programming-paradigm",permalink:"/docs/development/Programming-paradigm",draft:!1,editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/docs/development/model.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"Programming-paradigm",title:"Programming Paradigm",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Version Upgrade Guide",permalink:"/docs/user-guide/version-upgrade"},next:{title:"Project Configuration",permalink:"/docs/development/config"}},m={},g=[{value:"Architecture",id:"architecture",level:2},{value:"Programming paradigm",id:"programming-paradigm",level:2},{value:"DataStream",id:"datastream",level:3},{value:"Flink Sql",id:"flink-sql",level:3},{value:"TableEnvironment",id:"tableenvironment",level:4},{value:"StreamTableEnvironment",id:"streamtableenvironment",level:4},{value:"RunTime Context",id:"runtime-context",level:2},{value:"StreamingContext",id:"streamingcontext",level:3},{value:"TableContext",id:"tablecontext",level:3},{value:"StreamTableContext",id:"streamtablecontext",level:3},{value:"Life Cycle",id:"life-cycle",level:2},{value:"Life Cycle - init",id:"life-cycle---init",level:3},{value:"Life Cycle \u2014 config",id:"life-cycle--config",level:3},{value:"Life Cycle \u2014 ready",id:"life-cycle--ready",level:3},{value:"Life Cycle \u2014 handle",id:"life-cycle--handle",level:3},{value:"Life Cycle \u2014 start",id:"life-cycle--start",level:3},{value:"Life Cycle \u2014 destroy",id:"life-cycle--destroy",level:3},{value:"Catalog Structure",id:"catalog-structure",level:2},{value:"Packaged Deployment",id:"packaged-deployment",level:2},{value:"Start command",id:"start-command",level:2}],c={toc:g},d="wrapper";function u(e){let{components:n,...l}=e;return(0,r.yg)(d,(0,t.A)({},c,l,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"There are some rules and conventions to be followed in any framework. Only by following and mastering these rules can we use them more easily and achieve twice the result with half the effort.When we develop Flink job, we actually use the API provided by Flink to write an executable program (which must have a ",(0,r.yg)("inlineCode",{parentName:"p"},"main()")," function) according to the development method required by Flink. We access various",(0,r.yg)("inlineCode",{parentName:"p"},"Connector"),"in the program, and after a series of ",(0,r.yg)("inlineCode",{parentName:"p"},"operator"),"operations, we finally sink the data to the target storage through the ",(0,r.yg)("inlineCode",{parentName:"p"},"Connector")," ."),(0,r.yg)("p",null,'We call this method of step-by-step programming according to certain agreed rules the "programming paradigm". In this chapter, we will talk about the "programming paradigm" of StreamPark and the development considerations.'),(0,r.yg)("p",null,"Let's start from these aspects"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Architecture"),(0,r.yg)("li",{parentName:"ul"},"Programming paradigm"),(0,r.yg)("li",{parentName:"ul"},"RunTime Context"),(0,r.yg)("li",{parentName:"ul"},"Life Cycle"),(0,r.yg)("li",{parentName:"ul"},"Catalog Structure"),(0,r.yg)("li",{parentName:"ul"},"Packaged Deployment")),(0,r.yg)("h2",{id:"architecture"},"Architecture"),(0,r.yg)("p",null,(0,r.yg)("img",{src:a(60941).A,width:"2734",height:"1311"})),(0,r.yg)("h2",{id:"programming-paradigm"},"Programming paradigm"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"streampark-core")," is positioned as a programming time framework, rapid development scaffolding, specifically created to simplify Flink development. Developers will use this module during the development phase. Let's take a look at what the programming paradigm of ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"Flink Sql")," with StreamPark looks like, and what the specifications and requirements are."),(0,r.yg)("h3",{id:"datastream"},"DataStream"),(0,r.yg)("p",null,"StreamPark provides both ",(0,r.yg)("inlineCode",{parentName:"p"},"scala")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"Java")," APIs to develop ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," programs, the specific code development is as follows."),(0,r.yg)(i.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"scala",label:"Scala",default:!0,mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"import org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.flink.api.scala._\n\nobject MyFlinkApp extends FlinkStreaming {\n\n    override def handle(): Unit = {\n        ...\n    }\n}\n"))),(0,r.yg)(o.A,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},'public class MyFlinkJavaApp {\n\n    public static void main(String[] args) {\n        StreamEnvConfig JavaConfig = new StreamEnvConfig(args, (environment, parameterTool) -> {\n            //The user can set parameters for the environment...\n            System.out.println("environment argument set...");\n        });\n\n        StreamingContext context = new StreamingContext(JavaConfig);\n\n        ....\n\n        context.start();\n    }\n}\n')))),(0,r.yg)("p",null,"To develop with the ",(0,r.yg)("inlineCode",{parentName:"p"},"scala")," API, the program must inherit from ",(0,r.yg)("inlineCode",{parentName:"p"},"FlinkStreaming"),". After inheritance, it is mandatory for developers to implement the ",(0,r.yg)("inlineCode",{parentName:"p"},"handle()")," method, which is the entry point for users to write code, and the ",(0,r.yg)("inlineCode",{parentName:"p"},"streamingContext")," for developers to use."),(0,r.yg)("p",null,"Development with the ",(0,r.yg)("inlineCode",{parentName:"p"},"Java")," API can not omit the ",(0,r.yg)("inlineCode",{parentName:"p"},"main()")," method due to the limitations of the language itself, so it will be a standard ",(0,r.yg)("inlineCode",{parentName:"p"},"main()")," function,. The user needs to create the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext")," manually. ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext")," is a very important class, which will be introduced later."),(0,r.yg)("admonition",{title:"tip",type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"The above lines of ",(0,r.yg)("inlineCode",{parentName:"p"},"scala")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"Java")," code are the basic skeleton code necessary to develop ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," with StreamPark. Developing a ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," program with StreamPark. Starting from these lines of code, Java API development requires the developer to manually start the task ",(0,r.yg)("inlineCode",{parentName:"p"},"start"),".")),(0,r.yg)("h3",{id:"flink-sql"},"Flink Sql"),(0,r.yg)("p",null,"The TableEnvironment is used to create the contextual execution environment for Table & SQL programs and is the entry point for Table & SQL programs. The main functions of the TableEnvironment include: interfacing with external systems, registering and retrieving tables and metadata, executing SQL statements, and providing more detailed configuration options."),(0,r.yg)("p",null,"The Flink community has been promoting the batch processing capability of DataStream and unifying the stream-batch integration, and in Flink 1.12, the stream-batch integration is truly unified, many historical APIs such as: DataSet API, BatchTableEnvironment API, etc. are deprecated and retired from the history stage. TableEnvironment",(0,r.yg)("strong",{parentName:"p"}," and "),"StreamTableEnvironment**."),(0,r.yg)("p",null," StreamPark provides a more convenient API for the development of ",(0,r.yg)("strong",{parentName:"p"},"TableEnvironment")," and ",(0,r.yg)("strong",{parentName:"p"},"StreamTableEnvironment")," environments."),(0,r.yg)("h4",{id:"tableenvironment"},"TableEnvironment"),(0,r.yg)("p",null,"To develop Table & SQL jobs, TableEnvironment will be the recommended entry class for Flink, supporting both Java API and Scala API, the following code demonstrates how to develop a TableEnvironment type job in StreamPark"),(0,r.yg)(i.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"scala",label:"Scala",default:!0,mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"import org.apache.streampark.flink.core.scala.FlinkTable\n\nobject TableApp extends FlinkTable {\n\n    override def handle(): Unit = {\n    ...\n    }\n\n}\n"))),(0,r.yg)(o.A,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},'import org.apache.streampark.flink.core.scala.TableContext;\nimport org.apache.streampark.flink.core.scala.util.TableEnvConfig;\n\npublic class JavaTableApp {\n\n    public static void main(String[] args) {\n        TableEnvConfig tableEnvConfig = new TableEnvConfig(args, null);\n        TableContext context = new TableContext(tableEnvConfig);\n        ...\n        context.start("Flink SQl Job");\n    }\n}\n')))),(0,r.yg)("admonition",{title:"tip",type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"The above lines of Scala and Java code are the essential skeleton code for developing a TableEnvironment with StreamPark.\nScala API must inherit FlinkTable, Java API development needs to manually construct TableContext, and the developer needs to manually start the task ",(0,r.yg)("inlineCode",{parentName:"p"},"start"),".")),(0,r.yg)("h4",{id:"streamtableenvironment"},"StreamTableEnvironment"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," is used in stream computing scenarios, where the object of stream computing is a ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream"),". Compared to ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvironment"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," provides an interface to convert between ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"Table"),". If your application is written using the ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream API")," in addition to the ",(0,r.yg)("inlineCode",{parentName:"p"},"Table API")," & ",(0,r.yg)("inlineCode",{parentName:"p"},"SQL"),", you need to use the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment"),".\nThe following code demonstrates how to develop a ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," type job in StreamPark."),(0,r.yg)(i.A,{mdxType:"Tabs"},(0,r.yg)(o.A,{value:"scala",label:"Scala",default:!0,mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"package org.apache.streampark.test.tablesql\n\nimport org.apache.streampark.flink.core.scala.FlinkStreamTable\n\nobject StreamTableApp extends FlinkStreamTable {\n\n  override def handle(): Unit = {\n    ...\n  }\n\n}\n"))),(0,r.yg)(o.A,{value:"Java",label:"Java",mdxType:"TabItem"},(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},'\nimport org.apache.streampark.flink.core.scala.StreamTableContext;\nimport org.apache.streampark.flink.core.scala.util.StreamTableEnvConfig;\n\npublic class JavaStreamTableApp {\n\n    public static void main(String[] args) {\n        StreamTableEnvConfig JavaConfig = new StreamTableEnvConfig(args, null, null);\n        StreamTableContext context = new StreamTableContext(JavaConfig);\n\n        ...\n\n        context.start("Flink SQl Job");\n    }\n}\n')))),(0,r.yg)("admonition",{title:"tip",type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"The above lines of scala and Java code are the essential skeleton code for developing ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," with StreamPark, and for developing ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," programs with StreamPark. Starting from these lines of code, Java code needs to construct ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableContext")," manually, and ",(0,r.yg)("inlineCode",{parentName:"p"},"Java API")," development requires the developer to start the task ",(0,r.yg)("inlineCode",{parentName:"p"},"start")," manually.")),(0,r.yg)("h2",{id:"runtime-context"},"RunTime Context"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"RunTime Context")," - ",(0,r.yg)("strong",{parentName:"p"},"StreamingContext")," , ",(0,r.yg)("strong",{parentName:"p"},"TableContext")," , ",(0,r.yg)("strong",{parentName:"p"},"StreamTableContext")," are three very important objects in StreamPark, next we look at the definition and role of these three ",(0,r.yg)("strong",{parentName:"p"},"Context"),"."),(0,r.yg)("center",null,(0,r.yg)("img",{src:"/doc/image/streampark_coreapi.png",width:"60%"})),(0,r.yg)("h3",{id:"streamingcontext"},"StreamingContext"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext")," inherits from ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment"),", adding ",(0,r.yg)("inlineCode",{parentName:"p"},"ParameterTool")," on top of ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment"),", which can be simply understood as:"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"StreamingContext")," = ",(0,r.yg)("strong",{parentName:"p"},"ParameterTool")," + ",(0,r.yg)("strong",{parentName:"p"},"StreamExecutionEnvironment")),(0,r.yg)("p",null,"The specific definitions are as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"class StreamingContext(val parameter: ParameterTool, private val environment: StreamExecutionEnvironment)\n    extends StreamExecutionEnvironment(environment.getJavaEnv) {\n\n  /**\n   * for scala\n   *\n   * @param args\n   */\n  def this(args: (ParameterTool, StreamExecutionEnvironment)) = this(args._1, args._2)\n\n  /**\n   * for Java\n   *\n   * @param args\n   */\n  def this(args: StreamEnvConfig) = this(FlinkStreamingInitializer.initJavaStream(args))\n\n  ...\n}\n")),(0,r.yg)("admonition",{title:"tip",type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"This object is very important and will be used throughout the lifecycle of the task in the ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," job. The ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext")," itself inherits from the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment"),", and the configuration file is fully integrated into the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext"),", so that it is very easy to get various parameters from the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext"),".")),(0,r.yg)("p",null,"In StreamPark, ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext")," is also the entry class for the Java API to write ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," jobs, one of the constructors of ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext")," is specially built for the Java API, the constructor is defined as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"/**\n * for Java\n * @param args\n*/\ndef this(args: StreamEnvConfig) = this(FlinkStreamingInitializer.initJavaStream(args))\n")),(0,r.yg)("p",null,"From the above constructor you can see that to create ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingContext"),", you need to pass in a ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamEnvConfig")," object. ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamEnvConfig")," is defined as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"class StreamEnvConfig(val args: Array[String], val conf: StreamEnvConfigFunction)\n")),(0,r.yg)("p",null,"In the constructor of StreamEnvConfig,"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"args")," is the start parameter and must be the ",(0,r.yg)("inlineCode",{parentName:"li"},"args")," in the ",(0,r.yg)("inlineCode",{parentName:"li"},"main")," method"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"conf")," is a ",(0,r.yg)("inlineCode",{parentName:"li"},"Function")," of type ",(0,r.yg)("inlineCode",{parentName:"li"},"StreamEnvConfigFunction"))),(0,r.yg)("p",null,"The definition of ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamEnvConfigFunction")," is as follows."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"@FunctionalInterface\npublic interface StreamEnvConfigFunction {\n    /**\n     * Used to initialize the StreamExecutionEnvironment, for the function can be implemented, customize the parameters to be set...\n     *\n     * @param environment\n     * @param parameterTool\n     */\n    void configuration(StreamExecutionEnvironment environment, ParameterTool parameterTool);\n}\n")),(0,r.yg)("p",null,"The purpose of the ",(0,r.yg)("inlineCode",{parentName:"p"},"Function")," is to allow the developer to set more parameters by means of hooks, which will pass the ",(0,r.yg)("inlineCode",{parentName:"p"},"parameter")," (parsing all parameters in the configuration file) and the initialized ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment")," object to the developer to set more parameters, e.g.:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},'StreamEnvConfig JavaConfig = new StreamEnvConfig(args, (environment, parameterTool) -> {\n    System.out.println("environment argument set...");\n    environment.getConfig().enableForceAvro();\n});\n\nStreamingContext context = new StreamingContext(JavaConfig);\n')),(0,r.yg)("h3",{id:"tablecontext"},"TableContext"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"TableContext")," inherits from ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvironment"),". On top of ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvironment"),", it adds ",(0,r.yg)("inlineCode",{parentName:"p"},"ParameterTool"),", which is used to create the contextual execution environment for ",(0,r.yg)("inlineCode",{parentName:"p"},"Table")," & ",(0,r.yg)("inlineCode",{parentName:"p"},"SQL")," programs. It can be simply understood as :"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"TableContext")," = ",(0,r.yg)("strong",{parentName:"p"},"ParameterTool")," + ",(0,r.yg)("strong",{parentName:"p"},"TableEnvironment")),(0,r.yg)("p",null,"The specific definitions are as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"class TableContext(val parameter: ParameterTool,\n                   private val tableEnv: TableEnvironment)\n                   extends TableEnvironment\n                   with FlinkTableTrait {\n\n  /**\n   * for scala\n   *\n   * @param args\n   */\n  def this(args: (ParameterTool, TableEnvironment)) = this(args._1, args._2)\n\n  /**\n   * for Java\n   * @param args\n   */\n  def this(args: TableEnvConfig) = this(FlinkTableInitializer.initJavaTable(args))\n\n  ...\n}\n")),(0,r.yg)("p",null,"In StreamPark, ",(0,r.yg)("inlineCode",{parentName:"p"},"TableContext")," is also the entry class for the Java API to write ",(0,r.yg)("inlineCode",{parentName:"p"},"Table Sql")," jobs of type ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvironment"),". One of the constructor methods of ",(0,r.yg)("inlineCode",{parentName:"p"},"TableContext")," is a constructor specifically built for the ",(0,r.yg)("inlineCode",{parentName:"p"},"Java API"),", which is defined as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"\n/**\n* for Java\n* @param args\n*/\ndef this(args: TableEnvConfig) = this(FlinkTableInitializer.initJavaTable(args))\n")),(0,r.yg)("p",null,"From the above constructor you can see that to create a ",(0,r.yg)("inlineCode",{parentName:"p"},"TableContext"),", you need to pass in a ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvConfig")," object. ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvConfig")," is defined as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"class TableEnvConfig(val args: Array[String], val conf: TableEnvConfigFunction)\n")),(0,r.yg)("p",null,"In the constructor method of TableEnvConfig,"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"args")," is the start parameter, and is the ",(0,r.yg)("inlineCode",{parentName:"li"},"args")," in the ",(0,r.yg)("inlineCode",{parentName:"li"},"main")," method."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"conf")," is a ",(0,r.yg)("inlineCode",{parentName:"li"},"Function")," of type ",(0,r.yg)("inlineCode",{parentName:"li"},"TableEnvConfigFunction"))),(0,r.yg)("p",null,"The definition of ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvConfigFunction")," is as follows."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},"@FunctionalInterface\npublic interface TableEnvConfigFunction {\n    /**\n     * Used to initialize the TableEnvironment, for the function can be implemented, customize the parameters to be set...\n     *\n     * @param tableConfig\n     * @param parameterTool\n     */\n    void configuration(TableConfig tableConfig, ParameterTool parameterTool);\n\n}\n")),(0,r.yg)("p",null,"The purpose of the ",(0,r.yg)("inlineCode",{parentName:"p"},"Function")," is to allow the developer to set more parameters by hooking the ",(0,r.yg)("inlineCode",{parentName:"p"},"parameter")," (parsing all parameters in the configuration file) and the ",(0,r.yg)("inlineCode",{parentName:"p"},"TableConfig")," object in the initialized ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvironment")," to the developer to set more parameters, such as:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},'TableEnvConfig config = new TableEnvConfig(args,(tableConfig,parameterTool)->{\n    tableConfig.setLocalTimeZone(ZoneId.of("Asia/Shanghai"));\n});\nTableContext context = new TableContext(config);\n...\n')),(0,r.yg)("h3",{id:"streamtablecontext"},"StreamTableContext"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableContext")," inherits from ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," and is used in stream computing scenarios. The object of stream computation is ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream"),". Compared to ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvironment"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," provides an interface for conversion between ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"Table"),".\n",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableContext")," adds ",(0,r.yg)("inlineCode",{parentName:"p"},"ParameterTool")," on top of ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," and directly accesses the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment")," API, which can be easily understood as:"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"StreamTableContext")," = ",(0,r.yg)("strong",{parentName:"p"},"ParameterTool")," + ",(0,r.yg)("strong",{parentName:"p"},"StreamTableEnvironment")," + ",(0,r.yg)("strong",{parentName:"p"},"StreamExecutionEnvironment")),(0,r.yg)("p",null,"The specific definitions are as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"\nclass StreamTableContext(val parameter: ParameterTool,\n                         private val streamEnv: StreamExecutionEnvironment,\n                         private val tableEnv: StreamTableEnvironment)\n                         extends StreamTableEnvironment\n                         with FlinkTableTrait {\n\n  /**\n   * Once the Table is converted to a DataStream,\n   * The DataStream job must be executed using the execute method of the StreamExecutionEnvironment.\n   */\n  private[scala] var isConvertedToDataStream: Boolean = false\n\n  /**\n   * for scala\n   *\n   * @param args\n   */\n  def this(args: (ParameterTool, StreamExecutionEnvironment, StreamTableEnvironment)) =\n  this(args._1, args._2, args._3)\n\n  /**\n   * for Java\n   *\n   * @param args\n   */\n  def this(args: StreamTableEnvConfig) = this(FlinkTableInitializer.initJavaStreamTable(args))\n  ...\n}\n")),(0,r.yg)("p",null,"In StreamPark, ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableContext")," is the entry class for the Java API to write ",(0,r.yg)("inlineCode",{parentName:"p"},"Table Sql")," jobs of type ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvironment"),". One of the constructors of ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableContext")," is a function built specifically for the Java API, which is defined as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"\n  /**\n   * for Java\n   *\n   * @param args\n   */\ndef this(args: StreamTableEnvConfig) = this(FlinkTableInitializer.initJavaStreamTable(args))\n")),(0,r.yg)("p",null,"From the above constructor you can see that to create ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableContext"),", you need to pass in a ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvConfig")," object. ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableEnvConfig")," is defined as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"class StreamTableEnvConfig (\n    val args: Array[String],\n    val streamConfig: StreamEnvConfigFunction,\n    val tableConfig: TableEnvConfigFunction\n)\n")),(0,r.yg)("p",null,"The constructor of StreamTableEnvConfig has three parameters:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"args")," is the start parameter, and must be ",(0,r.yg)("inlineCode",{parentName:"li"},"args")," in the ",(0,r.yg)("inlineCode",{parentName:"li"},"main")," method"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"streamConfig")," is a ",(0,r.yg)("inlineCode",{parentName:"li"},"Function")," of type ",(0,r.yg)("inlineCode",{parentName:"li"},"StreamEnvConfigFunction"),"."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"tableConfig")," is a ",(0,r.yg)("inlineCode",{parentName:"li"},"Function")," of type ",(0,r.yg)("inlineCode",{parentName:"li"},"TableEnvConfigFunction"))),(0,r.yg)("p",null,"The definitions of ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamEnvConfigFunction")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvConfigFunction")," have been described above and will not be repeated here."),(0,r.yg)("p",null,"The purpose of this ",(0,r.yg)("inlineCode",{parentName:"p"},"Function")," is to allow the developer to set more parameters by means of hooks. Unlike the other parameter settings above, this ",(0,r.yg)("inlineCode",{parentName:"p"},"Function")," provides the opportunity to set both the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment")," and the ",(0,r.yg)("inlineCode",{parentName:"p"},"TableEnvironment"),", which will pass the ",(0,r.yg)("inlineCode",{parentName:"p"},"parameter")," and the initialized ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment ' and the "),"TableConfig",(0,r.yg)("inlineCode",{parentName:"p"},"object in the"),"TableEnvironment` are passed to the developer for additional parameter settings, such as:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-java"},'\nStreamTableEnvConfig JavaConfig = new StreamTableEnvConfig(args, (environment, parameterTool) -> {\n    environment.getConfig().enableForceAvro();\n}, (tableConfig, parameterTool) -> {\n    tableConfig.setLocalTimeZone(ZoneId.of("Asia/Shanghai"));\n});\n\nStreamTableContext context = new StreamTableContext(JavaConfig);\n\n...\n')),(0,r.yg)("admonition",{title:"tip",type:"info"},(0,r.yg)("p",{parentName:"admonition"},"You can use the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment")," ",(0,r.yg)("inlineCode",{parentName:"p"},"API")," directly in the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamTableContext"),", ",(0,r.yg)("strong",{parentName:"p"},"methods prefixed with $")," are the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment")," API."),(0,r.yg)("p",{parentName:"admonition"},(0,r.yg)("img",{src:a(81429).A,width:"2024",height:"1378"}))),(0,r.yg)("h2",{id:"life-cycle"},"Life Cycle"),(0,r.yg)("p",null,"The lifecycle concept is currently only available for the ",(0,r.yg)("inlineCode",{parentName:"p"},"scala")," API. this lifecycle explicitly defines the entire process of running a task, which is executed according to this lifecycle as long as it is inherited from ",(0,r.yg)("inlineCode",{parentName:"p"},"FlinkStreaming")," or ",(0,r.yg)("inlineCode",{parentName:"p"},"FlinkTable")," or ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamingTable"),". The core methods of the lifecycle are as follows."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-scala"},"\n final def main(args: Array[String]): Unit = {\n    init(args)\n    ready()\n    handle()\n    jobExecutionResult = context.start()\n    destroy()\n  }\n\n  private[this] def init(args: Array[String]): Unit = {\n    SystemPropertyUtils.setAppHome(KEY_APP_HOME, classOf[FlinkStreaming])\n    context = new StreamingContext(FlinkStreamingInitializer.initStream(args, config))\n  }\n\n  /**\n   * Users can override the sub-method...\n   *\n   */\n  def ready(): Unit = {}\n\n  def config(env: StreamExecutionEnvironment, parameter: ParameterTool): Unit = {}\n\n  def handle(): Unit\n\n  def destroy(): Unit = {}\n\n")),(0,r.yg)("p",null,"The life cycle is as follows."),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"init"),"          Stages of configuration file initialization"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"config"),"        Stage of manual parameter setting by the developer"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"ready"),"         Stage for executing custom actions before starting"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"handle"),"        Stages of developer code access"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"start"),"         Stages of program initiation"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"destroy"),"       Stages of destruction")),(0,r.yg)("p",null,(0,r.yg)("img",{alt:"Life Cycle",src:a(29695).A,width:"1790",height:"1560"})),(0,r.yg)("h3",{id:"life-cycle---init"},"Life Cycle - init"),(0,r.yg)("p",null,"In the ",(0,r.yg)("strong",{parentName:"p"},"init")," phase, the framework automatically parses the incoming configuration file and initializes the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment")," according to the various parameters defined inside. This step is automatically executed by the framework and does not require developer involvement."),(0,r.yg)("h3",{id:"life-cycle--config"},"Life Cycle \u2014 config"),(0,r.yg)("p",null,"The purpose of the ",(0,r.yg)("strong",{parentName:"p"},"config")," phase is to allow the developer to set more parameters (other than the agreed configuration file), by means of hooks. The ",(0,r.yg)("strong",{parentName:"p"},"config")," phase passes ",(0,r.yg)("inlineCode",{parentName:"p"},"parameter")," (all parameters in the configuration file parsed in the ",(0,r.yg)("em",{parentName:"p"},"init")," phase) and the ",(0,r.yg)("inlineCode",{parentName:"p"},"StreamExecutionEnvironment")," object initialized in the ",(0,r.yg)("em",{parentName:"p"},"init")," phase to the developer,this allows the developer to configure more parameters."),(0,r.yg)("admonition",{title:"Description",type:"note"},(0,r.yg)("p",{parentName:"admonition"},"The ",(0,r.yg)("strong",{parentName:"p"},"config")," stage is an optional stage that requires developer participation.")),(0,r.yg)("h3",{id:"life-cycle--ready"},"Life Cycle \u2014 ready"),(0,r.yg)("p",null,"The ",(0,r.yg)("strong",{parentName:"p"},"ready")," stage is an entry point for the developer to do other actions after the parameters have been set, and is done after ",(0,r.yg)("strong",{parentName:"p"},"initialization is complete "),"before the ",(0,r.yg)("strong",{parentName:"p"},"program is started"),"."),(0,r.yg)("admonition",{title:"Description",type:"note"},(0,r.yg)("p",{parentName:"admonition"},"The ",(0,r.yg)("strong",{parentName:"p"},"ready")," stage is a stage that requires developer participation and is optional.")),(0,r.yg)("h3",{id:"life-cycle--handle"},"Life Cycle \u2014 handle"),(0,r.yg)("p",null,"The ",(0,r.yg)("strong",{parentName:"p"},"handle")," stage is the stage of accessing the code written by the developer, it is the entrance to the code written by the developer and is the most important stage, this ",(0,r.yg)("inlineCode",{parentName:"p"},"handle")," method will force the developer to implement."),(0,r.yg)("admonition",{title:"Description",type:"note"},(0,r.yg)("p",{parentName:"admonition"},"The ",(0,r.yg)("strong",{parentName:"p"},"handle")," stage is a mandatory stage that requires developer participation.")),(0,r.yg)("h3",{id:"life-cycle--start"},"Life Cycle \u2014 start"),(0,r.yg)("p",null,"The ",(0,r.yg)("strong",{parentName:"p"},"start")," phase, which starts the task, is executed automatically by the framework."),(0,r.yg)("h3",{id:"life-cycle--destroy"},"Life Cycle \u2014 destroy"),(0,r.yg)("p",null,"The ",(0,r.yg)("strong",{parentName:"p"},"destroy")," phase is the last phase before jvm exits after the program has finished running, and is generally used to wrap up the work."),(0,r.yg)("admonition",{title:"Description",type:"note"},(0,r.yg)("p",{parentName:"admonition"},"The ",(0,r.yg)("strong",{parentName:"p"},"destroy")," stage is an optional stage that requires developer participation.")),(0,r.yg)("h2",{id:"catalog-structure"},"Catalog Structure"),(0,r.yg)("p",null,"The recommended project directory structure is as follows, please refer to the directory structure and configuration in ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/apache/incubator-streampark-quickstart"},"StreamPark-flink-quickstart")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-tree"},".\n|\u2500\u2500 assembly\n\u2502    \u251c\u2500\u2500 bin\n\u2502    \u2502    \u251c\u2500\u2500 startup.sh                             //Launch Script\n\u2502    \u2502    \u251c\u2500\u2500 setclasspath.sh                        //ava environment variables related to the script (internal use of the framework, developers do not need to pay attention to)\n\u2502    \u2502    \u251c\u2500\u2500 shutdown.sh                            //Task stop script (not recommended)\n\u2502    \u2502    \u2514\u2500\u2500 flink.sh                               //the script that internal use to, when starting (this script is used internally in the framework, the developer does not need to pay attention to)\n\u2502    \u2502\u2500\u2500 conf\n\u2502    \u2502    \u251c\u2500\u2500 test\n\u2502    \u2502    \u2502    \u251c\u2500\u2500 application.yaml                  //Configuration file for the test phase\n\u2502    \u2502    \u2502    \u2514\u2500\u2500 sql.yaml                          //flink sql\n\u2502    \u2502    \u2502\n\u2502    \u2502    \u251c\u2500\u2500 prod\n\u2502    \u2502    \u2502    \u251c\u2500\u2500 application.yaml                  //Profiles for the production (prod) stage\n\u2502    \u2502    \u2502    \u2514\u2500\u2500 sql.yaml                          //flink sql\n\u2502    \u2502\u2500\u2500 logs                                        //logs Catalog\n\u2502    \u2514\u2500\u2500 temp\n\u2502\n\u2502\u2500\u2500 src\n\u2502    \u2514\u2500\u2500 main\n\u2502         \u251c\u2500\u2500 Java\n\u2502         \u251c\u2500\u2500 resources\n\u2502         \u2514\u2500\u2500 scala\n\u2502\n\u2502\u2500\u2500 assembly.xml\n\u2502\n\u2514\u2500\u2500 pom.xml\n")),(0,r.yg)("p",null,"assembly.xml is the configuration file needed for the assembly packaging plugin, defined as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-xml"},"<assembly>\n    <id>bin</id>\n    <formats>\n        <format>tar.gz</format>\n    </formats>\n    <fileSets>\n        <fileSet>\n            <directory>assembly/bin</directory>\n            <outputDirectory>bin</outputDirectory>\n            <fileMode>0755</fileMode>\n        </fileSet>\n        <fileSet>\n            <directory>${project.build.directory}</directory>\n            <outputDirectory>lib</outputDirectory>\n            <fileMode>0755</fileMode>\n            <includes>\n                <include>*.jar</include>\n            </includes>\n            <excludes>\n                <exclude>original-*.jar</exclude>\n            </excludes>\n        </fileSet>\n        <fileSet>\n            <directory>assembly/conf</directory>\n            <outputDirectory>conf</outputDirectory>\n            <fileMode>0755</fileMode>\n        </fileSet>\n        <fileSet>\n            <directory>assembly/logs</directory>\n            <outputDirectory>logs</outputDirectory>\n            <fileMode>0755</fileMode>\n        </fileSet>\n        <fileSet>\n            <directory>assembly/temp</directory>\n            <outputDirectory>temp</outputDirectory>\n            <fileMode>0755</fileMode>\n        </fileSet>\n    </fileSets>\n</assembly>\n")),(0,r.yg)("h2",{id:"packaged-deployment"},"Packaged Deployment"),(0,r.yg)("p",null,"The recommended packaging mode in ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/apache/incubator-streampark-quickstart/tree/dev/quickstart-flink"},"streampark-flink-quickstart")," is recommended. It runs ",(0,r.yg)("inlineCode",{parentName:"p"},"maven package")," directly to generate a standard StreamPark recommended project package, after unpacking the directory structure is as follows."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-text"},".\nStreamPark-flink-quickstart-1.0.0\n\u251c\u2500\u2500 bin\n\u2502   \u251c\u2500\u2500 startup.sh                             //Launch Script\n\u2502   \u251c\u2500\u2500 setclasspath.sh                        //Java environment variable-related scripts (used internally, not of concern to users)\n\u2502   \u251c\u2500\u2500 shutdown.sh                            //Task stop script (not recommended)\n\u2502   \u251c\u2500\u2500 flink.sh                               //Scripts used internally at startup (used internally, not of concern to the user)\n\u251c\u2500\u2500 conf\n\u2502   \u251c\u2500\u2500 application.yaml                       //Project's configuration file\n\u2502   \u251c\u2500\u2500 sql.yaml                               // flink sql file\n\u251c\u2500\u2500 lib\n\u2502   \u2514\u2500\u2500 StreamPark-flink-quickstart-1.0.0.jar     //The project's jar package\n\u2514\u2500\u2500 temp\n")),(0,r.yg)("h2",{id:"start-command"},"Start command"),(0,r.yg)("p",null,"The application.yaml and sql.yaml configuration files need to be defined before starting. If the task to be started is a ",(0,r.yg)("inlineCode",{parentName:"p"},"DataStream")," task, just follow the configuration file directly after startup.sh."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"bin/startup.sh --conf conf/application.yaml\n")),(0,r.yg)("p",null,"If the task you want to start is the ",(0,r.yg)("inlineCode",{parentName:"p"},"Flink Sql")," task, you need to follow the configuration file and sql.yaml."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"bin/startup.sh --conf conf/application.yaml --sql conf/sql.yaml\n")))}u.isMDXComponent=!0},81429:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/streampark_apis-574b985bc649f26aacf875ccd0155793.jpeg"},29695:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/streampark_scala_life_cycle-7bfdf4f1228b36f59fae79943d5c8108.png"},60941:(e,n,a)=>{a.d(n,{A:()=>t});const t=a.p+"assets/images/streampark_archite-ff9eba80347b8b3c47d241007386f7bc.png"}}]);